{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introductory applied machine learning (INFR10069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"5\">Your Score was 46.0 out of a total of 95.0, or 48.4%</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Data analysis and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "**It is important that you follow the instructions below to the letter - we will not be responsible for incorrect marking due to non-standard practices.**\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/michael-camilleri/IAML2018) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate py3iaml\n",
    "cd [DIRECTORY CONTAINING GIT REPOSITORY]\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Read the instructions carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the code/markdown cell where to put it\n",
    "\n",
    "1. There are some questions which are **specific to those taking the Level-11 version** of the course (INFR11182 and INFR11152). These are clearly marked with the words **(LEVEL 11)** and must be completed by those taking the Level 11 course. Those on the Level 10 version (INFR10069) may (and are advised to) attempt such questions but this will not affect their mark in any way, nor will they get feedback on them.\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (i.e. use the `datasets` directory **adjacent** to this file).\n",
    "\n",
    "1. Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation.\n",
    "\n",
    "1. Make sure to show **all** your code/working. \n",
    "\n",
    "1. Write readable code. While we do not expect you to follow [PEP8](https://www.python.org/dev/peps/pep-0008/) to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. **Do** use inline comments when doing something non-standard. When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer. Marks *WILL* be deducted if the marker cannot understand your logic/results.\n",
    "\n",
    "1. **Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you should NOT borrow actual text or code from other students. We ask that you provide a list of the people who you've had discussions with (if any).\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "\n",
    "**IMPORTANT:** You must submit this assignment by **Thursday 18/10/2018 at 16:00**. \n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics is that normally you will not be allowed to submit coursework late. See the [ITO webpage](http://web.inf.ed.ac.uk/infweb/student-services/ito/admin/coursework-projects/late-coursework-extension-requests) for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Resubmission:** If you submit your file again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline.\n",
    "\n",
    "All submissions happen electronically. To submit:\n",
    "\n",
    "1. Fill out this notebook, and save it, making sure to **KEEP the name of the file UNCHANGED**.\n",
    "\n",
    "1. On a DICE environment, open the terminal, navigate to the location of this notebook, and submit this notebook file using the following command:\n",
    "\n",
    "  ```submit iaml cw1 \"01_Analysis.ipynb\"```\n",
    "\n",
    "  What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You can check the status of your submissions with the `show_submissions` command.\n",
    "  \n",
    "1. **Distance Learners:** To copy your work onto DICE (so that you can use the `submit` command) you can use `scp` or `rsync` (you may need to install these yourself). You can copy files to `student.ssh.inf.ed.ac.uk`, then ssh into it in order to submit. The following is an example (replace entries in `[square brackets]` with your specific details):\n",
    "```\n",
    "filename=\"01_Analysis.ipynb\"\n",
    "local_scp_filepath=[DIRECTORY CONTAINING GIT REPOSITORY]${filename}\n",
    "server_address=student.ssh.inf.ed.ac.uk\n",
    "scp -r ${local_scp_filepath} [YOUR USERNAME]@${server_address}:${filename}\n",
    "# rsync -rl ${local_scp_filepath} [YOUR USERNAME]@${server_address}:${filename}\n",
    "ssh [YOUR USERNAME]@${server_address}\n",
    "ssh student.login\n",
    "submit iaml cw1 \"01_Analysis.ipynb\"\n",
    "```\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "The Level 10 and Level 11 points are marked out of different totals, however these are all normalised to 100%.\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n",
    "\n",
    "Note that while this is not a programming assignment, in questions which involve visualisation of results and/or long cold snippets, some marks may be deducted if the code is not adequately readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Execute the cell below to import all packages you will be using in the rest of the assignemnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3iaml/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.plotter import scatter_jitter, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the dataset\n",
    "\n",
    "This assignment is based on the 20 Newsgroups Dataset. This dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware, comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale, soc.religion.christian). \n",
    "\n",
    "There are three versions of the 20 Newsgroups Dataset. In this assignment we will use the `bydate` matlab version in which documents are sorted by date into training (60%) and test (40%) sets, newsgroup-identifying headers are dropped and duplicates are removed. This collection comprises roughly 61,000 different words, which results in a bag-of-words representation with frequency counts. More specifically, each document is represented by a 61,000 dimensional vector that contains the counts for each of the 61,000 different words present in the respective document. \n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We will use documents from only 5 out of the 20 newsgroups, which results in a 5-class problem. The class is conveniently stored in the `class` column. More specifically the 5 classes correspond to the following newsgroups: \n",
    "1. `alt.atheism`\n",
    "2. `comp.sys.ibm.pc.hardware`\n",
    "3. `comp.sys.mac.hardware`\n",
    "4. `rec.sport.baseball`\n",
    "5. `rec.sport.hockey `\n",
    "\n",
    "However, note here that classes 2-3 and 4-5 are rather closely related. Additionally, we computed the [mutual information](https://en.wikipedia.org/wiki/Mutual_information) of each word with the class attribute and selected the some words out of 61,000 that had highest mutual information. For very sophisticated technical reasons (which you should know!) 1 was added to all the word counts in part 1. The resulting representation is much more compact and can be used directly to perform our experiments in Python.\n",
    "\n",
    "**Hint**: The data was preprocessed by a very busy PhD student... and hence should never be taken to be perfect at face value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploration of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to get a feel for the data that you will be dealing with in the rest of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 --- [10 marks] ==========\n",
    "\n",
    "1. [Code] Load the dataset `raw_20news.csv` into a data-frame called `news_raw`. Using pandas methods we learnt in class, extract some basic information about the data. \n",
    "\n",
    "1. [Text] In a short paragraph, summarise the key features of the dataset. *Hint: Look at what we did in the labs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">7.0/10.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2129 entries, 0 to 2256\n",
      "Columns: 521 entries, w1_aaa to class\n",
      "dtypes: int64(521)\n",
      "memory usage: 8.5 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1_aaa</th>\n",
       "      <th>w2_pins</th>\n",
       "      <th>w3_kmr</th>\n",
       "      <th>w4_notion</th>\n",
       "      <th>w5_queens</th>\n",
       "      <th>w6_dwyer</th>\n",
       "      <th>w7_defenseman</th>\n",
       "      <th>w8_gld</th>\n",
       "      <th>w9_tocchet</th>\n",
       "      <th>w10_home</th>\n",
       "      <th>...</th>\n",
       "      <th>w512_constantly</th>\n",
       "      <th>w513_generate</th>\n",
       "      <th>w514_definite</th>\n",
       "      <th>w515_lacks</th>\n",
       "      <th>w516_combination</th>\n",
       "      <th>w517_sitting</th>\n",
       "      <th>w518_surface</th>\n",
       "      <th>w519_fashion</th>\n",
       "      <th>w520_sit</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.00000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.128229</td>\n",
       "      <td>6.097698</td>\n",
       "      <td>6.120244</td>\n",
       "      <td>5.551902</td>\n",
       "      <td>5.521841</td>\n",
       "      <td>6.12776</td>\n",
       "      <td>5.633161</td>\n",
       "      <td>6.090653</td>\n",
       "      <td>5.970409</td>\n",
       "      <td>5.624706</td>\n",
       "      <td>...</td>\n",
       "      <td>9.666510</td>\n",
       "      <td>9.217473</td>\n",
       "      <td>9.061531</td>\n",
       "      <td>9.398309</td>\n",
       "      <td>9.175200</td>\n",
       "      <td>9.708783</td>\n",
       "      <td>8.807891</td>\n",
       "      <td>9.719587</td>\n",
       "      <td>9.307656</td>\n",
       "      <td>3.092532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38.292577</td>\n",
       "      <td>46.190953</td>\n",
       "      <td>46.656022</td>\n",
       "      <td>40.953913</td>\n",
       "      <td>40.978098</td>\n",
       "      <td>45.96434</td>\n",
       "      <td>41.146918</td>\n",
       "      <td>45.762060</td>\n",
       "      <td>44.266628</td>\n",
       "      <td>40.769105</td>\n",
       "      <td>...</td>\n",
       "      <td>45.844064</td>\n",
       "      <td>43.948910</td>\n",
       "      <td>40.969185</td>\n",
       "      <td>43.833064</td>\n",
       "      <td>42.403283</td>\n",
       "      <td>47.294120</td>\n",
       "      <td>39.341038</td>\n",
       "      <td>46.185082</td>\n",
       "      <td>45.059367</td>\n",
       "      <td>1.395948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>572.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>591.000000</td>\n",
       "      <td>600.00000</td>\n",
       "      <td>546.000000</td>\n",
       "      <td>591.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>587.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>599.000000</td>\n",
       "      <td>585.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>597.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 521 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            w1_aaa      w2_pins       w3_kmr    w4_notion    w5_queens  \\\n",
       "count  2129.000000  2129.000000  2129.000000  2129.000000  2129.000000   \n",
       "mean      5.128229     6.097698     6.120244     5.551902     5.521841   \n",
       "std      38.292577    46.190953    46.656022    40.953913    40.978098   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "50%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "max     572.000000   583.000000   579.000000   580.000000   591.000000   \n",
       "\n",
       "         w6_dwyer  w7_defenseman       w8_gld   w9_tocchet     w10_home  \\\n",
       "count  2129.00000    2129.000000  2129.000000  2129.000000  2129.000000   \n",
       "mean      6.12776       5.633161     6.090653     5.970409     5.624706   \n",
       "std      45.96434      41.146918    45.762060    44.266628    40.769105   \n",
       "min       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "25%       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "50%       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "75%       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "max     600.00000     546.000000   591.000000   578.000000   578.000000   \n",
       "\n",
       "          ...       w512_constantly  w513_generate  w514_definite  \\\n",
       "count     ...           2129.000000    2129.000000    2129.000000   \n",
       "mean      ...              9.666510       9.217473       9.061531   \n",
       "std       ...             45.844064      43.948910      40.969185   \n",
       "min       ...              1.000000       1.000000       1.000000   \n",
       "25%       ...              3.000000       2.000000       3.000000   \n",
       "50%       ...              5.000000       5.000000       5.000000   \n",
       "75%       ...              7.000000       7.000000       7.000000   \n",
       "max       ...            590.000000     587.000000     577.000000   \n",
       "\n",
       "        w515_lacks  w516_combination  w517_sitting  w518_surface  \\\n",
       "count  2129.000000       2129.000000   2129.000000   2129.000000   \n",
       "mean      9.398309          9.175200      9.708783      8.807891   \n",
       "std      43.833064         42.403283     47.294120     39.341038   \n",
       "min       1.000000          1.000000      1.000000      1.000000   \n",
       "25%       3.000000          2.000000      3.000000      3.000000   \n",
       "50%       5.000000          5.000000      5.000000      5.000000   \n",
       "75%       7.000000          7.000000      7.000000      7.000000   \n",
       "max     598.000000        568.000000    599.000000    585.000000   \n",
       "\n",
       "       w519_fashion     w520_sit        class  \n",
       "count   2129.000000  2129.000000  2129.000000  \n",
       "mean       9.719587     9.307656     3.092532  \n",
       "std       46.185082    45.059367     1.395948  \n",
       "min        1.000000     1.000000     1.000000  \n",
       "25%        3.000000     2.000000     2.000000  \n",
       "50%        5.000000     4.000000     3.000000  \n",
       "75%        7.000000     6.000000     4.000000  \n",
       "max      600.000000   597.000000     5.000000  \n",
       "\n",
       "[8 rows x 521 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = os.path.join(os.getcwd(), 'datasets', 'raw_20news.csv')     # loads the dataset into a data-frame\n",
    "news_raw = pd.read_csv(news)                                       # reads the data-frame\n",
    "news_raw.info()                                                    # basic information about variables\n",
    "news_raw.describe()                                                # displays summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">4.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key features of this dataset includes, it analyses 520 words. The dataset also uses 8.5 MB  and is of type int64. Looking at the summary statistics we see that the count is 2129 which means that there are 2129 entries. By looking at the standard deviation of the classess we see that it is 1.40 (rounded to 1dp), which is a low standard deviation, this ulimatley suggests that there isn't an even distrubtion of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">Most values are close to 1, and almost all attributes have some extreme values</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 ---  [6 marks] ==========\n",
    "1. [Code] Display the names of some of the attributes in the training datset. \n",
    "1. [Text] Describe the output and comment (1 or 2 sentences) keeping in mind the selection procedure for the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">1.0/6.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2247</th>\n",
       "      <th>2248</th>\n",
       "      <th>2249</th>\n",
       "      <th>2250</th>\n",
       "      <th>2251</th>\n",
       "      <th>2252</th>\n",
       "      <th>2253</th>\n",
       "      <th>2254</th>\n",
       "      <th>2255</th>\n",
       "      <th>2256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>w1_aaa</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2_pins</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w3_kmr</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w4_notion</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w5_queens</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w6_dwyer</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w7_defenseman</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>399</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w8_gld</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w9_tocchet</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w10_home</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0     1     2     3     4     5     6     7     8     9     \\\n",
       "w1_aaa            1     1     1     1     1     1     1     1     1     1   \n",
       "w2_pins           1     1     1     1     1     1     1     1     1     1   \n",
       "w3_kmr            1     1     1     1     1     1     1     1     1     1   \n",
       "w4_notion         1     1     1     1     1     1     1     1     1     1   \n",
       "w5_queens         1     1     1     1     1     1     2     1     1     1   \n",
       "w6_dwyer          1     1     1     1     1     1     1     1     1     1   \n",
       "w7_defenseman     1     1     1     1     1     1     1     1     1     1   \n",
       "w8_gld            1     1     1     1     1     1     1     1     1     1   \n",
       "w9_tocchet        1     1     1     1     1     1     1     1     1     1   \n",
       "w10_home          1     1     2     1     1     1     1     1     1     1   \n",
       "\n",
       "               ...   2247  2248  2249  2250  2251  2252  2253  2254  2255  \\\n",
       "w1_aaa         ...      1   506     1     1     1     1     1     1     1   \n",
       "w2_pins        ...      1   124     1     1     1     1     1     1     1   \n",
       "w3_kmr         ...      1   381     1     1     1     1     1     1     1   \n",
       "w4_notion      ...      1   290     1     1     1     1     1     1     1   \n",
       "w5_queens      ...      1   298     1     1     1     1     1     1     1   \n",
       "w6_dwyer       ...      1   167     1     1     1     1     1     1     1   \n",
       "w7_defenseman  ...      1   399     1     1     3     1     1     1     1   \n",
       "w8_gld         ...      1   525     1     1     1     1     1     1     1   \n",
       "w9_tocchet     ...      1   184     1     1     1     1     1     1     1   \n",
       "w10_home       ...      1   201     1     1     1     1     1     1     2   \n",
       "\n",
       "               2256  \n",
       "w1_aaa            1  \n",
       "w2_pins           1  \n",
       "w3_kmr            1  \n",
       "w4_notion         1  \n",
       "w5_queens         1  \n",
       "w6_dwyer          1  \n",
       "w7_defenseman     1  \n",
       "w8_gld            1  \n",
       "w9_tocchet        1  \n",
       "w10_home          1  \n",
       "\n",
       "[10 rows x 2129 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_raw.T.head(10)                                               # display names of a few attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">1.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#COMMENT# just the names ideally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, I can see that there are 10 documents we are analysing. As the table is transposed, the rows represent words and each entry is the freqeuncy the word appears in the document.\n",
    "\n",
    "The words here selected are where you can see a correlation to a class quite quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">0.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">Should comment on how the features are named - w[n]_[word], where n is an index, and word is a word. Also - some words appear not to be real words (e.g. “gld”) but are highly informative of class</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 --- [4 marks] ==========\n",
    "Familiarise yourself with the [`stripplot`](https://seaborn.github.io/generated/seaborn.stripplot.html?highlight=stripplot#seaborn.stripplot) function in `seaborn`. \n",
    "\n",
    "1. [Code] Pick one attribute of your choice (except `class`) and display a stripplot for that attribute for dataset **A**. Demonstrate the distribution of the data separately for each class (by making appropriate use of the `x` argument in `stripplot`). Set the `jitter` argument to `True` and the `alpha` argument to an appropriate value (to add transparency). When the jitter parameter is enabled a small amount of noise is added to the data so that there is less overlap and the distribution is easier to visualise. \n",
    "\n",
    "1. [Text] Mention anything peculiar you observe in the Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">4.0/4.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x axis is class, y axis is word \n",
    "plt.subplots(figsize=(10,8))\n",
    "\n",
    "# create a strip plot\n",
    "sns.stripplot(x='class',y='w153_law', data=news_raw, jitter=True, alpha=0.4)\n",
    "\n",
    "# title for graph\n",
    "plt.title('Frequency of the word law in different classes')                           \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)\n",
    "\n",
    "Observing the graph, we see the likely word frequency is one in the documents, this could mean that the word does not occur in the document. However I can also see that they may be some outliers above 100. As depending on the word count in each document, I think it is unlikley that there would be 600 word frequency in a document. This suggests that there are outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4  --- [8 marks] ==========\n",
    "Let us analyse this further. The stripplot illustrates the distribution of a single attribute. We can also visualise the joint distribution of two variables by using a scatter plot. Again, we want to add a bit of jitter into the data so that is easier to see which parts of the space (2-dimensional in our case) have larger probability densities. For this, you will be using the function `scatter_jitter` from the `utils` library which we provided. This function takes as input two numpy arrays containing the features of interest. \n",
    "\n",
    "1. [Code] First, pick two attributes of your choice from dataset A and use the provided function to plot their joint distribution. Do this twice (i.e. pick 4 attributes and do plots between two pairs: you do not need to plot between each possible pair). You can play around with the amount of noise added by tweaking the `jitter` parameter. Alternatively, you can just use its default value which is set to 0.2. Label the axes appropriately.\n",
    "\n",
    "1. [Text] Do these plots increase or reduce the possibility of the data-set containing outliers? Why or why not? How would you visualise the data to verify this? **N.B. There is no need to code anything for this part, just indicate a reasonable type of plot, and what you expect to see.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0/8.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,8))\n",
    "\n",
    "# plot the scatter plot\n",
    "scatter_jitter(news_raw['w153_law'],news_raw['w5_queens'], jitter = 0.3)          \n",
    "\n",
    "# set the labels and title\n",
    "plt.xlabel('w153_law')                                                            \n",
    "plt.ylabel('w5_queens')                                                           \n",
    "plt.title('Word frequency distrubtion between law and queen')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">0.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">no plots shown, cell not run</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these plots increase the possibility of us visualising the data to see outliers. As shown above we can see the majority of the points are in the left corner of the plot, as of this I would consider the rest of the points to be outliers. I think they are outliers as I would not think queens are law are closely related or likely to be in a document together.\n",
    "\n",
    "I think a better way to visualise if outliers do exist would be a box plot as you can see the spread of data a lot easier from the min value and the max value of each word as the higher the spread, the more likely the data contains outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">you would need to check/visualise the values of more than just one or two attributes per document, to see if all attributes are extreme for these potential outliers</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 --- [15 marks] ==========\n",
    "We want to get rid of the potential outliers in the data. Your job now is to create a new dataset from `news_raw` (name it `news_clean`) and remove the outliers.\n",
    "\n",
    "1. [Text] Decide on a criterion to find the outliers and argue why it is reasonable. \n",
    "1. [Code] Then implement it and visualise the result (in terms of your criterion). **Be careful** not to alter the original dataset as we will need it later. \n",
    "1. [Text] Finally report the number of points in the cleaned dataset, and the number rejected as outliers and comment on the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">7.0/15.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first assume a normal disturbtion. I think this is reasonable as there are going to be a few words which have have a similar occurence while there are also going to be words which have a very high frequency and low frequency. I decided to assume a normal disturbtion as I first tried to find outliers using interquartile range, however this is not a viable soloution due to most of interquartile range of words being zero.\n",
    "\n",
    "Assuming a normal distrubtion and taking three standard deviations we can then assume that approximatley 99.7% of the data lies within this range. This then means any extreme values will be considered outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">this seems to exclude data points based on the value of only one of their attributes - a more knowledgeable proposal would take into account the values of all the attributes - e.g. by setting a cut-off on the sum of all the attribute values.\n",
    "the 65%, 95%, 99.7% statistics are only true if your data is normally distributed - you can check this with a histogram. the frequencies you describe do not match what i see in the data - most word frequencies are 1 or close to 1, none are lower, and a few are much higher.</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "std = news_raw.describe().T['std']                                                              # gets the standard deviation of the words               \n",
    "mean = news_raw.describe().T['mean']                                                            # gets the mean of the words of the words\n",
    "three_std = 2* std                                                                              # calculates 3 standard deviations of the words \n",
    "\n",
    "mini = mean - three_std                                                                         # calculates the minimum value for threshold\n",
    "maxi = mean + three_std                                                                         # calculates the maximum value for threshold\n",
    "\n",
    "news_clean = news_raw[(mini < news_raw) & (news_raw < maxi)].dropna()                           # removes outliers and drops it \n",
    "display(news_clean.head(10))                                                                    # prints out news_clean\n",
    "\n",
    "\n",
    "\n",
    "# visualising the word 'law'again, the graph shows a more realistic distrubution with less extreme values\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,8))\n",
    "sns.stripplot(x='class',y='w153_law', data=news_clean, jitter=True, alpha=0.4, ax=ax[0])        # prints out class of law\n",
    "\n",
    "\n",
    "# visualising correlation between two words, graphs shows no extreme values\n",
    "scatter_jitter(news_clean['w124_innings'],news_clean['w190_penalty'], jitter = 0.3)             # plot the scatter plot\n",
    "plt.xlabel('w124_innings')                                                                      # set the x axis \n",
    "plt.ylabel('w190_peanlty')                                                                      # set the y axis \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Attempt at trying IQR to find outliers \n",
    "# desc = news_raw.describe().T\n",
    "# q_one = desc['25%']\n",
    "# q_three = desc['75%']\n",
    "# iqr = q_three - q_one\n",
    "# iqr_range =iqr  * 1.5\n",
    "\n",
    "# mini = q_one - iqr_range\n",
    "# maxi = q_three + iqr_range\n",
    "\n",
    "# news_clean = news_raw[(mini <= news_raw) & (news_raw <= maxi)].dropna()\n",
    "# news_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">cell not run, no visualisations, no reporting of number of outliers/inliers.\n",
    "you seem to actually use 2 standard deviations, not 3.</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of documents before outliers is 2129.\n",
    "- The number of documents before after outliers is 2094.\n",
    "\n",
    "This means that there is a difference of 35 documents. There was one document which stood out for me, it was document 2248. Just by looking at the word frequency of words, I could clearly see it was an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.6 --- (LEVEL 11) --- [10 marks] ==========\n",
    "\n",
    "1. [Code] Visualise some of the outlier documents and some of the inlier ones. \n",
    "1. [Text] Comment on the observations. Also comment on whether it is appropriate to do such cleaning on just the training data or on the entire data-set (including testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes classification\n",
    "Now we want to fit a Gaussian Naive Bayes model to the cleaned dataset. You might want first to familiarise yourself with the [`GaussianNB`](http://scikit-learn.org/0.19/modules/generated/sklearn.naive_bayes.GaussianNB.html) class in `Sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 --- [6 marks] ==========\n",
    "\n",
    "Load the cleaned datasets `train_20news.csv` and `test_20news.csv` into pandas dataframes `news_train` and `news_test` respectively. Using pandas summary methods, confirm that the data is similar in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">4.0/6.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train = pd.read_csv(os.path.join(os.getcwd(), 'datasets', 'train_20news.csv'))     # Load and read training data\n",
    "news_test  = pd.read_csv(os.path.join(os.getcwd(), 'datasets', 'test_20news.csv'))      # Load and read test data\n",
    "\n",
    "news_train.info()                                                                       # displays info of training data \n",
    "\n",
    "print(\"\")  \n",
    "\n",
    "news_test.info()                                                                        # displays info of training data\n",
    "display(news_train.describe())                                                          # displays summary statistics \n",
    "display(news_test.describe())                                                           # displays summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">4.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">You could also use .info()</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 --- [4 marks] ==========\n",
    "\n",
    "[Text] Answer (in brief) the following two questions:\n",
    "1. What is the assumption behing the Naive Bayes Model?\n",
    "1. What would be the main issue we would have to face if we didn't make this assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0/4.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The assumption of using the Naive Bayes Model is independence of data features with each other within the class.\n",
    "\n",
    "2) The main issue faced if this assumption is not made is computing the probabilities of the dependence on each other as this would mean the features have a non-zero covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">would need to estimate full joint distribution between all variables - exponential in number of variables, too many numbers to estimate</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 --- [8 marks] ==========\n",
    "\n",
    "1. [Code] By using the `scatter_jitter` function, display a scatter plot of the features `w281_ico` and `w273_tek` for the **cleaned** dataset A. Set the jitter value to an appropriate value for visualisation. Label axes appropriately.\n",
    "1. [Text] What do you observe about these two features? Does this impact the validity of the Naive Bayes assumption? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0/8.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(9,7))\n",
    "scatter_jitter(news_clean['w281_ico'], news_clean['w273_tek'],jitter = 0.3)                 # plot the graph\n",
    "\n",
    "plt.xlabel('w281_ico')                                                                      # set the x axis \n",
    "plt.ylabel('w273_tek')                                                                      # set the y axis \n",
    "\n",
    "plt.title('Word frequency distrubtion between tek and ico')    \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">0.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">cell not run, no plot</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the graph above, there is a positive correlation between the words 'tek' and 'ico'\n",
    "\n",
    "- I think this impacts the validity of the Naive Bayes assumption as there is a strong correlation between the two words. It looks as if 'tek' is there, then 'ico' is also likely to appear. However just because there is an correlation does not imply the two words are independent from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">NB assumption talks about conditional independence - without viewing the classes, no way of knowing whether these variables are conditionally independent or not.</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 --- [7 marks] ==========\n",
    "1. [Text] What is a reasonable baseline against which to compare the classiffication performance? *Hint: What is the simplest classiffier you can think of?*. \n",
    "1. [Code] Estimate the baseline performance on the *training* data in terms of classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">7.0/7.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest classiffier is random selection. As there are 5 classifiers which we can assume to be evenly distrubuted such as a uniform distbution,then there is a 20% chance of having a class. If Naive Bayes accuracy rate is higher than 20 % then we can assume that the classification has a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier                                                     # import DummyClassifier\n",
    "             \n",
    "dummy_classifier = sklearn.dummy.DummyClassifier(strategy='most_frequent')                    # using method of most frequent\n",
    "\n",
    "dummy_classifier.fit(news_train.loc[:, news_train.columns != 'class'], news_train['class'])   # fit the model of the training data\n",
    "\n",
    "predict = dummy_classifier.predict(news_train.loc[:, news_train.columns != 'class'])          # predict labels for training data\n",
    "acc = accuracy_score(news_train['class'],predict)                                             # calculate the accuracy score\n",
    "\n",
    "print('Dummy Classifier has %0.2f%% accuracy rate.'% (acc*100))                               # displays accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">5.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_2_5'></a>\n",
    "### ========== Question 2.5 --- [12 marks] ==========\n",
    "\n",
    "1. [Code] Fit a Gaussian Naive Bayes model to the cleaned dataset. \n",
    "\n",
    "1. [Code] Report the classification accuracy on the **training** dataset and plot a Confusion Matrix for the result (labelling the axes appropriately).\n",
    "\n",
    "1. [Text] Comment on the performance of the model. Is the accuracy a reasonable metric to use for this dataset?\n",
    "\n",
    "*Hint: You may make use of utility functions we provided, as well as an sklearn method for computing confusion matrices*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0/12.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_classifier = GaussianNB()                                                                # create a classifier\n",
    "\n",
    "g_classifier.fit(news_train.loc[:, news_train.columns != 'class'], news_train['class'])    # fit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">3.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predict = g_classifier.predict(news_train.loc[:, news_train.columns != 'class'])     # predict the test data\n",
    "\n",
    "accuracy = accuracy_score(news_train['class'],predict)                               # calculates accuracy of Gaussian\n",
    "print('GaussianNB has %0.2f%% accuracy rate on training data.'% (accuracy*100))      # displays accuracy\n",
    "\n",
    "cm = confusion_matrix(news_train['class'],predict)                                   # creates a confusion matrix\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "\n",
    "#--- gives the values in the matrix\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "plt.imshow(cm, cmap = 'Blues')                                                       # plot the confusion matrix\n",
    "plt.ylabel('True labels')                                                            # set the x axis \n",
    "plt.xlabel('Predicted labels')                                                       # set the y axis\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">0.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">cell not run, no accuracy/confusion matrix/class densities are reported</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy from the Gaussian classifier is approximatley 87% which is a high accuracy rate. I think the accuracy is a reasonable metric to use for this dataset as, this is predicting the class of the document, so either it could be the right classification or not as there are only 5 options it could be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">0.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">should compare to baseline; notice that classes are balanced so accuracy is a reasonable metric; notice that potentially accuracy isn’t great if it’s important to us to get one particular class right</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.6 --- [3 marks] ==========\n",
    "\n",
    "[Text] Comment on the confusion matrix from the previous question. Does it look like what you would have expected? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">1.0/3.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows high accuracy which is what I would have expected considering it is accuracy rate of 87%. There does seem to be quite a few documents (value =149) which have a true label as 2 but was predicted as 3 which is quite intresting as it shows how both classes are quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">1.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">should notice that the classes that are confused are related - comp.sys.ibm.pc.hardware and comp.sys.mac.hardware - or which classes are best/worst predicted.</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.7 --- [12 marks] ==========\n",
    "\n",
    "Now we want to evaluate the generalisation of the classifier on new (i.e. unseen data). \n",
    "\n",
    "1. [Code] Use the classifier you trained in Question [2.5](#question_2_5) (i.e. on the cleaned dataset) and test its performance on the test dataset. Display classification accuracy and plot a confusion matrix of the performance on the test data. \n",
    "\n",
    "1. [Code] Also, reevaluate the performance of the baseline on the test data.\n",
    "\n",
    "1. [Text] In a short paragraph (3-4 sentences) compare and comment on the results with the training data/baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">4.0/12.0</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = g_classifier.predict(news_test.loc[:, news_test.columns != 'class'])\n",
    "\n",
    "accuracy = accuracy_score(news_test['class'],predict_test)                               # calculates accuracy of Gaussian\n",
    "print('GaussianNB has %0.2f%% accuracy rate on test data.'% (accuracy*100))                                                                    # displays accuracy\n",
    "\n",
    "\n",
    "cm = confusion_matrix(news_test['class'],predict_test)                                   # creates a confusion matrix\n",
    "plt.figure(figsize = (7,7))\n",
    "\n",
    "#--- gives the values in the matrix\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.imshow(cm, cmap = 'Blues')                                                            # plot the confusion matrix\n",
    "plt.ylabel('True labels')                                                                 # set the x axis \n",
    "plt.xlabel('Predicted labels')                                                            # set the y axis\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">2.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">cell not run, nothing reported</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dummy = dummy_classifier.predict(news_test.loc[:, news_test.columns != 'class'])  # predict labels for training data\n",
    "\n",
    "acc_test = accuracy_score(news_test['class'],predict_dummy)                               # calculate the accuracy score\n",
    "\n",
    "print('Dummy classifier has %0.2f%% accuracy rate on test data.'% (acc_test*100))         # displays accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">1.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">cell not run, nothing reported</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)\n",
    "\n",
    "- Baseline Results :\n",
    " - The baseline classifier had an accuracy rate of 21.3% for the training data while for the test data it had an accuracy rate of 20.3%. This here shows that the baseline was more accurate at classifying the training data than the test data.\n",
    "- Gaussian Results :\n",
    " - The Gaussian classifier had an accuracy rate of 87.8% for the training data while for the test data it had an accuracy rate of 82%. This shows that the Gaussian classifier was more accurate at classifying the training data than the test data.\n",
    "\n",
    "Overall the Gaussian classifier is more accurate than the baseline classifier which is as expected. I think something that surprised me is the fact that the training data had a higher accuracy in both classifiers than the test data, I think the reason is because we end up using the training data as the fit which means it would be more accurate for the training data than the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><font color=\"blue\" size=\"4\">1.0</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\"><font color=\"green\" size=\"4\">should mention:\n",
    "-- whether performance on test set is good/bad;\n",
    "-- easy/difficult classes to predict (by name);\n",
    "-- if model is overfitting or whether train-test split is balanced</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.8 --- (LEVEL 11) --- [7 marks] ==========\n",
    "1. [Code] Fit a Gaussian Naive Bayes model to the original raw dataset (including the outliers) and test its performance on the **test** set. \n",
    "\n",
    "1. [Text] Comment on the output and explain why or why not cleaning affects the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.9 --- (LEVEL 11) --- [3 marks] ==========\n",
    "\n",
    "In this exercise we have fitted a Gaussian Naive Bayes classifier to the data (i.e. the class conditional densities are Gaussians). However, this is not ideally suited to our dataset. Can you explain why this is so? what kind of Naive Bayes model would you employ to this kind of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
